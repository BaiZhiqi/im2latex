{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# im2latex(S): Data Pickling\n",
    "\n",
    "&copy; Copyright 2017 Sumeet S Singh\n",
    "\n",
    "    This file is part of im2latex solution by Sumeet S Singh.\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the Affero GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    Affero GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the Affero GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What This Program Does\n",
    "This notebook curates the [im2latex-100k dataset from Harvard NLP](https://zenodo.org/record/56198#.WT4fsxMrJE4) from a [Harvard NLP project](http://lstm.seas.harvard.edu/latex/) generated using [their code]( https://github.com/Miffyli/im2latex-dataset).\n",
    "\n",
    "* This notebook loads the data set which are png files generated using [textogif](https://www.fourmilab.ch/webtools/textogif/textogif.html) (via the abovementioned sourcecode)\n",
    "* Checks and visualizes the data\n",
    "* Curates the data - whitening etc.\n",
    "* Converts it to numpy arrays and finally stores it as pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "apJbCsBHl-2A"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import display, Image as ipImage\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 600\n",
    "pd.options.display.max_columns = 20\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.width = 160\n",
    "data_folder = '../data/generated2'\n",
    "image_folder = os.path.join(data_folder,'formula_images')\n",
    "raw_data_folder = os.path.join(data_folder,'raw_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%run commons.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_binned = pd.read_pickle(os.path.join(data_folder, 'df_binned.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_seq_bins(df_, data_dir_):\n",
    "    bin_lens = df_.bin_len.unique()\n",
    "    bins = {}\n",
    "    raw_data_dir = os.path.join(data_dir_, 'raw_data')\n",
    "    if not os.path.exists(raw_data_dir):\n",
    "        os.makedirs(raw_data_dir)\n",
    "        \n",
    "    for len_ in bin_lens:\n",
    "        df_slice = df_[df_.padded_seq_len == len_]\n",
    "        bin_ = np.array(df_slice.padded_seq.values.tolist(), dtype=np.int32)\n",
    "        assert bin_.shape[1] == len_\n",
    "        assert bin_.shape[0] == df_slice.shape[0]\n",
    "        bins[len_] = pd.DataFrame(bin_, index=df_slice.index)\n",
    "    ## Persist to disk\n",
    "    with open(os.path.join(raw_data_dir, 'seq_bins.pkl'), 'wb') as f:\n",
    "      pickle.dump(bins, f, pickle.HIGHEST_PROTOCOL)\n",
    "    return bins\n",
    "\n",
    "bins = make_seq_bins(df_binned, data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Whitening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_image_bins(df_, image_dir_, raw_data_dir_):\n",
    "    max_pixel = 255.0\n",
    "    max_height, max_width = df_binned[['height', 'width']].max().values\n",
    "    i = 1\n",
    "    images = []\n",
    "    for row in df_[['image','height','width']].itertuples():\n",
    "        ## Load image and convert to grayscale float32 values\n",
    "        im_ar = ndimage.imread(os.path.join(image_dir_,row[1]), flatten=True)\n",
    "        ## normalize values to lie between -1.0 and 1.0.\n",
    "        ## This is done in place of data whitening - i.e. normalizing to mean=0 and std-dev=0.5\n",
    "        ## Is is a very rough technique but legit for images\n",
    "        im_ar = (im_ar - max_pixel/2.0) / max_pixel\n",
    "        height, width = im_ar.shape\n",
    "        assert height == row[2]\n",
    "        assert width == row[3]\n",
    "        if (height < max_height) or (width < max_width):\n",
    "            ar = np.full((max_height, max_width), 0.5, dtype=np.float32)\n",
    "            h = (max_height-height)//2\n",
    "            ar[h:h+height, 0:width] = im_ar\n",
    "            im_ar = ar\n",
    "        images.append(im_ar)\n",
    "        i += 1\n",
    "    ## Attach the pandas index to the array so that we can keep track of the image IDs\n",
    "    pd2np_id = pd.Series(range(len(images)), index=df_.index)\n",
    "    pd2np_id.to_pickle(os.path.join(raw_data_dir_, 'pd2np_id.pkl'))\n",
    "    images = np.asarray(images)\n",
    "    with open(os.path.join(raw_data_dir_, 'padded_images.pkl'), 'wb') as f:\n",
    "        pickle.dump(images, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pan_im = make_image_bins(df_binned, image_folder, raw_data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(pan_im[0][0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBdkjESPK8tw"
   },
   "source": [
    "Data Whitening\n",
    "--------------\n",
    "\n",
    "Now let's load the data in a more manageable format. Since, depending on your computer setup you might not be able to fit it all in memory, we'll load each class into a separate dataset, store them on disk and curate them independently. Later we'll merge them into a single dataset of manageable size.\n",
    "\n",
    "We'll convert the entire dataset into a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road. \n",
    "\n",
    "A few images might not be readable, we'll just skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 30
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 399874,
     "status": "ok",
     "timestamp": 1444485886378,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "h7q0XhG3MJdf",
    "outputId": "92c391bb-86ff-431d-9ada-315568a19e59"
   },
   "outputs": [],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  num_images = 0\n",
    "  for image in image_files:\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[num_images, :, :] = image_data\n",
    "      num_images = num_images + 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUdbskYE2d87"
   },
   "source": [
    "---\n",
    "Check 2\n",
    "---------\n",
    "\n",
    "Let's verify that the data still looks good. Displaying a sample of the labels and images from the ndarray.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(\"Problem2\", (10.9, 5))\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(2,10), axes_pad=(0.1, 0.5))\n",
    "\n",
    "def displayOneImageFromPickle(ppath, pos, label):\n",
    "    global grid\n",
    "    try:\n",
    "      with open(ppath, 'rb') as f:\n",
    "        arrs = pickle.load(f)\n",
    "        num_images = arrs.shape[0];\n",
    "        img_idx = np.random.randint(0, high=num_images)\n",
    "        arr = arrs[img_idx]\n",
    "        print(ppath, \" num_images = \", num_images)\n",
    "        print(ppath,\"[\", img_idx, \"] shape = \", np.shape(arr), \" dtype = \", arr.dtype, \", mean,min,max = \", np.mean(arr), np.min(arr), np.max(arr))\n",
    "        fig.add_axes\n",
    "        grid[pos].set_title(label)\n",
    "        grid[pos].imshow(arr, cmap=\"gray\")\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', ppath, ':', e)\n",
    "      raise\n",
    "\n",
    "\n",
    "def verifyImageData(msg, pickle_filepaths, start_pos=0):\n",
    "    print(msg, pickle_filepaths)\n",
    "    end_pos = start_pos + len(pickle_filepaths)\n",
    "    labels = map(lambda path: os.path.splitext(os.path.basename(path))[0], pickle_filepaths)\n",
    "    print (labels)\n",
    "    map(displayOneImageFromPickle, pickle_filepaths, range(start_pos, end_pos), labels)\n",
    "    return end_pos\n",
    "\n",
    "def verifyImages():\n",
    "    pos = verifyImageData(\"Training Data Folders: \", train_datasets)\n",
    "    verifyImageData(\"Test Data Folders: \", test_datasets, pos)\n",
    "\n",
    "verifyImages()\n",
    "#plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LA7M7K22ynCt"
   },
   "source": [
    "Training, Validation and Test Datasets\n",
    "--------------------------------------\n",
    "\n",
    "Merge and prune the training data as needed. Depending on your computer setup, you might not be able to fit it all in memory, and you can tune `train_size` as needed. The labels will be stored into a separate array of integers 0 through 9.\n",
    "\n",
    "Also create a validation dataset for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 411281,
     "status": "ok",
     "timestamp": 1444485897869,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "s3mWgZLpyuzq",
    "outputId": "8af66da6-902d-4719-bedc-7c9fb7ae7948"
   },
   "outputs": [],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPTCnjIcyuKN"
   },
   "source": [
    "Next, we'll randomize the data. It's important to have the labels well shuffled for the training and test distributions to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "6WZ2l2tN2zOL"
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  print(\"randomize done\")\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)\n",
    "\n",
    "print(\"Randomization is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puDUTe6t6USl"
   },
   "source": [
    "---\n",
    "Check Again\n",
    "---------\n",
    "Convince yourself that the data is still good after shuffling!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J')\n",
    "\n",
    "def displayRandImagesFromArray(desc, arrs, labels, sample_size=10):\n",
    "    num_images = arrs.shape[0]\n",
    "    print(desc, \" num_images = \", num_images)\n",
    "    fig = plt.figure(figsize=(10.9, 5))\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(1,10), axes_pad=(0.1, 0.5))\n",
    "\n",
    "    for i in range(sample_size):\n",
    "        img_idx = np.random.randint(low=0, high=num_images)\n",
    "        arr = arrs[img_idx]\n",
    "        print(desc,\"[\", img_idx, \"] shape = \", np.shape(arr), \" dtype = \", arr.dtype,\n",
    "              \", mean,min,max = \", np.mean(arr), np.min(arr), np.max(arr))\n",
    "        grid[i].set_title(label_names[labels[img_idx]])\n",
    "        grid[i].imshow(arr, cmap=\"gray\")\n",
    "\n",
    "def displayRandImages():\n",
    "    displayRandImagesFromArray(\"Training Dataset: \", train_dataset, train_labels)\n",
    "    displayRandImagesFromArray(\"Test Dataset: \", test_dataset, test_labels)\n",
    "    displayRandImagesFromArray(\"Validation Dataset: \", valid_dataset, valid_labels)\n",
    "\n",
    "displayRandImages()\n",
    "#plt.close(\"all\")\n",
    "print(\"Done with data verification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIQJaJuwg5Hw"
   },
   "source": [
    "Finally, let's save the data for later reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "QiR_rETzem6C"
   },
   "outputs": [],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 413065,
     "status": "ok",
     "timestamp": 1444485899688,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "hQbLjrW_iT39",
    "outputId": "b440efc6-5ee1-4cbc-d02d-93db44ebd956"
   },
   "outputs": [],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)\n",
    "del train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gE_cRAQB33lk"
   },
   "source": [
    "---\n",
    "Check for Duplicates\n",
    "--------------------\n",
    "\n",
    "By construction, this dataset might contain a lot of overlapping samples, including training data that's also contained in the validation and test set! Overlap between training and test can skew the results if you expect to use your model in an environment where there is never an overlap, but are actually ok if you expect to see training samples recur when you use it.\n",
    "Measure how much overlap there is between training, validation and test samples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create unique validation and test sets\n",
    "\n",
    "def getFileList(root):\n",
    "    dirs = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J')\n",
    "    filelist = np.array([ os.path.join(dirpath, filename) for d in dirs for dirpath, dirnames, filenames in os.walk(root + '/' + d) for filename in filenames])\n",
    "    print(\"Files under %s = \"% root, filelist.shape)\n",
    "    n1 = filelist.size\n",
    "    filelist2 = np.unique(filelist)\n",
    "    n2 = filelist2.size\n",
    "    print(\"% Duplicates = \", ((n1-n2)*100.0)/n1)\n",
    "    return filelist\n",
    "\n",
    "train_files = getFileList('./notMNIST_large')\n",
    "test_files = getFileList('./notMNIST_small')\n",
    "\n",
    "del train_files\n",
    "del test_files\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Similar Images\n",
    "------------------------\n",
    "\n",
    "- What about near duplicates between datasets? (images that are almost identical)\n",
    "- Create a sanitized validation and test set, and compare your accuracy on those in subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadFlattenedData(pickle_file):\n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        print (\"loaded data \", data.viewkeys())\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "    x = data[\"train_dataset\"]\n",
    "    v = data[\"valid_dataset\"]\n",
    "    t = data[\"test_dataset\"]\n",
    "    \n",
    "    A = [a.reshape(a.shape[0],-1) for a in (x,v,t)]\n",
    "    \n",
    "    return A[0], A[1], A[2]\n",
    "\n",
    "x, v, t = loadFlattenedData('notMNIST.pickle')\n",
    "print(\"Test, Validate & Train Data Shapes = \", x.shape, v.shape, t.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "def calcDupes(m, desc):\n",
    "    # calculate normalized covariance matrix\n",
    "    m = np.mat(np.corrcoef(m))\n",
    "    n = m.shape[0]\n",
    "    similar = 0.95\n",
    "\n",
    "    # zero out the matrix diagonal and the triangle below it\n",
    "    m[np.tril_indices(m.shape[0])] = 0\n",
    "    assert np.trace(m) == 0\n",
    "    print(\"Min, Max, Median & SD of corrcoef of %s = \" % desc, np.nanmin(m), np.nanmax(m), np.nanmean(m), np.nanstd(m))\n",
    "\n",
    "    dupes = np.unique(np.argwhere(m >= similar)).shape[0]\n",
    "    #dupes = np.count_nonzero(m == 1)\n",
    "    print(\"Num duplicates in %s = %s = %s%%\" % (desc, dupes, dupes*100.0/n))\n",
    "\n",
    "def calcDupes2(m, desc):\n",
    "    print(\"Creating data frame for \", desc)\n",
    "    d = DataFrame({ i:m[i] for i in range(m.shape[0])})\n",
    "    print(\"Dataframe created. Calculating correlation matrix now ...\")\n",
    "    #s = DataFrame(d)\n",
    "    return d.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#v_dupes = calcDupes(v, \"v\")\n",
    "#v_dupes.to_pickle('v_dupes.pickle')\n",
    "t_dupes = calcDupes(t, \"t\")\n",
    "#t_dupes.to_pickle('t_dupes.pickle')\n",
    "print (\"Done calculating correlations of t\")\n",
    "#x_dupes = calcDupes2(x, \"x\")\n",
    "#x_dupes.to_pickle('x_dupes.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L8oww1s4JMQx"
   },
   "source": [
    "---\n",
    "Logistic Regression Classifier\n",
    "------------------------------\n",
    "\n",
    "Let's get an idea of what an off-the-shelf classifier can give you on this data. It's always good to check that there is something to learn, and that it's a problem that is not so trivial that a canned solution solves it.\n",
    "\n",
    "Train a simple model on this data using 50, 100, 1000 and 5000 training samples. Using the LogisticRegression model from sklearn.linear_model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Logistic Regression\n",
    "import sklearn.linear_model as glm\n",
    "\n",
    "def loadData(pickle_file):\n",
    "    data = {}\n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        print (\"loaded data \", data.viewkeys())\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "        \n",
    "    return data\n",
    "\n",
    "def learn(data, solver, num_train, multi_class):\n",
    "    num_validate = data[\"valid_dataset\"].shape[0]\n",
    "    num_test = data[\"test_dataset\"].shape[0]\n",
    "    #print(data)\n",
    "    train = {\n",
    "        'X': data[\"train_dataset\"][:num_train].reshape(num_train,-1),\n",
    "        'Y': data[\"train_labels\"][:num_train],\n",
    "        'vX': data[\"valid_dataset\"].reshape(num_validate,-1),\n",
    "        'vY': data[\"valid_labels\"],\n",
    "        'tX': data[\"test_dataset\"].reshape(num_test,-1),\n",
    "        'tY': data[\"test_labels\"]\n",
    "    }\n",
    "    \n",
    "    print(\"logr_%s.fit starting.\" % (solver))\n",
    "    print(\"shape X= \", train[\"X\"].shape, \" shape Y= \", train[\"Y\"].shape)\n",
    "    print(\"shape vX= \", train[\"vX\"].shape, \" shape vY= \", train[\"vY\"].shape)\n",
    "    print(\"shape tX= \", train[\"tX\"].shape, \" shape tY= \", train[\"tY\"].shape)\n",
    "    logr = glm.LogisticRegression(solver=solver,multi_class=multi_class, n_jobs=-1, warm_start=False)\n",
    "    logr.fit(train[\"X\"], train[\"Y\"])\n",
    "    print(\"logr_%s.fit done. Validation score = %s\" % (solver, logr.score(train[\"vX\"], train[\"vY\"])),\n",
    "         \"test score = \", logr.score(train[\"tX\"], train[\"tY\"]))\n",
    "    \n",
    "data = loadData('notMNIST.pickle')\n",
    "map( lambda n: learn(data, \"newton-cg\", n, \"multinomial\"), [50000])\n",
    "#map( lambda n: learn(data, \"lbfgs\", n, \"multinomial\"), [200000])\n",
    "#map( lambda n: learn(data, \"sag\", n, \"ovr\"), [200000])\n",
    "#map( lambda n: learn(data, \"liblinear\", n, \"ovr\"), [5000, 50000, 200000])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
