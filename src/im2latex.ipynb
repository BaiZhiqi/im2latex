{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# im2latex(S): Deep Learning Model\n",
    "\n",
    "&copy; Copyright 2017 Sumeet S Singh\n",
    "\n",
    "    This file is part of the im2latex solution (by Sumeet S Singh in particular since there are other solutions out there).\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the Affero GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    Affero GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the Affero GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "* Follows the [Show, Attend and Tell paper](https://www.semanticscholar.org/paper/Show-Attend-and-Tell-Neural-Image-Caption-Generati-Xu-Ba/146f6f6ed688c905fb6e346ad02332efd5464616)\n",
    "* [VGG ConvNet (16 or 19)](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) without the top-3 layers\n",
    "    * Pre-initialized with the VGG weights but allowed to train\n",
    "    * The ConvNet outputs $D$ dimensional vectors in a WxH grid where W and H are 1/16th of the input image size (due to 4 max-pool layers). Defining $W.H \\equiv L$ the ConvNet output represents L locations of the image $i \\in [1,L]$ and correspondingly outputs to L annotation vectors $a_i, i = [1,L]$ of size $D$.\n",
    "* A dense (FC) attention model: The deterministic soft-attention model of the paper computes $\\alpha_{t,i}$ which is used to select or blend the $a_i$ vectors before being fed as inputs to the decoder LSTM network (see below).\n",
    "    * Inputs to the attention model are $a_i$ and $h_{t-1}$ (previous hidden state of LSTM network - see below)\n",
    "    and $$\\alpha_{t,i} = softmax ( f_{att}(a_i, h_{t-1}) )$$\n",
    "* A Decoder model: A conditioned LSTM that outputs probabilities of the text tokens $y_t$ at each step. The LSTM is conditioned upon $z_t = \\sum_i^L(\\alpha_{t,i}.a_i)$ and takes the previous hidden state $h_{t-1}$ as input. In addition, an embedding of the previous output $Ey_{t-1}$ is also input to the LSTM. At training time, $y_{t-1}$ would be derived from the training samples, while at inferencing time it would be fed-back from the previous predicted word.\n",
    "    * $y$ is taken from a fixed vocabulary of K words. An embedding matrix $E$ is used to narrow its representation. The embedding weights $E$ are learnt end-to-end by the model as well.\n",
    "    * The decoder LSTM uses a deep layer between $h_t$ and $y_t$. It is called a deep output layer and is described in [section 3.2.2 of this paper](https://www.semanticscholar.org/paper/How-to-Construct-Deep-Recurrent-Neural-Networks-Pascanu-G%C3%BCl%C3%A7ehre/533ee188324b833e059cb59b654e6160776d5812). That is:\n",
    "    $$ p(y_t) = Softmax \\Big( f_out(Ey_{t-1}, h_t, \\hat{z}_t) \\Big) $$\n",
    "* Initialization MLPs: Two MLPs are used to produce the initial memory-state of the LSTM as well as $h_{t-1}$ value. Each MLP takes in the entire image's features (i.e. average of $a_i$) as its input and is trained end-to-end.\n",
    "    $$ c_o = f_{init,c}\\Big( \\sum_i^L a_i \\Big) $$\n",
    "    $$ h_o = f_{init,h}\\Big( \\sum_i^L a_i \\Big) $$\n",
    "* Training:\n",
    "    * All 4 components above are trained end-to-end using SGD\n",
    "    * The model is trained for a variable number of time steps - depending on each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Show, Attend and Tell\n",
    "    * [Paper](https://www.semanticscholar.org/paper/Show-Attend-and-Tell-Neural-Image-Caption-Generati-Xu-Ba/146f6f6ed688c905fb6e346ad02332efd5464616)\n",
    "    * [Slides](file:///Users/sumeet/im2latex/papers/Show,%20Attend%20and%20Tell:%20Neural%20Image%20Caption%20Generation%20with%20Visual%20Attention%20slides.pdf![image.png](attachment:image.png)\n",
    "1. [Simonyan, Karen and Andrew Zisserman. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” CoRR abs/1409.1556 (2014): n. pag.](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
    "1. [im2latex solution of Harvard NLP](http://lstm.seas.harvard.edu/latex/)\n",
    "1. [im2latex-dataset tools forked from Harvard NLP](https://github.com/untrix/im2latex-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dl_commons as dlc\n",
    "import tensorflow as tf\n",
    "from dl_commons import ParamDesc \n",
    "from dl_commons import mandatory\n",
    "from dl_commons import instanceof\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import LambdaCallback\n",
    "import keras\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = dlc.HyperParams((\n",
    "        ParamDesc('image_shape',\n",
    "                  'Shape of input images. Should be a python sequence.',\n",
    "                  ((120,1075,3),),\n",
    "                  (120,1075,3)\n",
    "                  ),\n",
    "        ParamDesc('batch_size',\n",
    "                  'batch size. Python integer.',\n",
    "                  (128,),\n",
    "                  128\n",
    "                  )\n",
    "            )).freeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Input Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def make_batch_list(df_, batch_size_):\n",
    "    ## Shuffle the samples\n",
    "    df_ = df_.sample(frac=1)\n",
    "    ## Make a list of batches\n",
    "    bin_lens = sorted(df_.bin_len.unique())\n",
    "    bin_counts = [df_[df_.bin_len==l].shape[0] for l in bin_lens]\n",
    "    batch_list = []\n",
    "    for i in range(len(bin_lens)):\n",
    "        bin_ = bin_lens[i]\n",
    "        num_batches = (bin_counts[i] // batch_size_)\n",
    "        ## Just making sure bin size is integral multiple of batch_size.\n",
    "        ## This is not a requirement for this function to operate, rather\n",
    "        ## is a way of possibly catching data-corrupting bugs\n",
    "        assert (bin_counts[i] % batch_size_) == 0\n",
    "        batch_list.extend([(bin_, j) for j in range(num_batches)])\n",
    "\n",
    "    np.random.shuffle(batch_list)\n",
    "    return batch_list\n",
    "\n",
    "class ShuffleIterator(object):\n",
    "    def __init__(self, df_, batch_size_):\n",
    "        self._df = df_\n",
    "        self._batch_size = batch_size_\n",
    "        self._batch_list = make_batch_list(self._df, batch_size_)\n",
    "        self._next_pos = 0\n",
    "        self._num_items = (df_.shape[0] // batch_size_)\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "#     def __iter__(self):\n",
    "#         return self\n",
    "    \n",
    "    def next(self):\n",
    "        ## This is an infinite iterator\n",
    "        with self.lock:\n",
    "            next_pos = self._next_pos\n",
    "            self._next_pos += 1\n",
    "            if next_pos >= self._num_items:\n",
    "                ## Recompose the batch-list\n",
    "                self._batch_list = make_batch_list(self._df, batch_size_)\n",
    "                next_pos = next_pos % self._num_items\n",
    "        \n",
    "        batch = self._batch_list[next_pos]\n",
    "        df_bin = self._df[self._df.bin_len == batch[0]]\n",
    "        assert df_bin.bin_len.iloc[batch[1]*self._batch_size] == batch[0]\n",
    "        assert df_bin.bin_len.iloc[(batch[1]+1)*self._batch_size-1] == batch[0]\n",
    "        return df_bin.iloc[batch[1]*self._batch_size:(batch[1]+1)*self._batch_size]\n",
    "\n",
    "class ImageIterator(ShuffleIterator):\n",
    "    def __init__(self, df_, batch_size_, image_dim_, image_dir_):\n",
    "        Shuffler.__init__(self, df_, batch_size_)\n",
    "        self._im_dim = image_dim_\n",
    "        self._image_dir = image_dir_\n",
    "\n",
    "    @staticmethod\n",
    "    def get_image_matrix(image_path_, height_, width_, padded_height_, padded_width_):\n",
    "        MAX_PIXEL = 255.0 # Ensure this is a float literal\n",
    "        ## Load image and convert to a 3-channel array\n",
    "        im_ar = ndimage.imread(os.path.join(image_dir_,sr_row_.image), mode='RGB')\n",
    "        ## normalize values to lie between -1.0 and 1.0.\n",
    "        ## This is done in place of data whitening - i.e. normalizing to mean=0 and std-dev=0.5\n",
    "        ## Is is a very rough technique but legit for images\n",
    "        im_ar = (im_ar - MAX_PIXEL/2.0) / MAX_PIXEL\n",
    "        height, width = im_ar.shape\n",
    "        assert height == height\n",
    "        assert width == width\n",
    "        if (height < padded_height_) or (width < padded_width_):\n",
    "            ar = np.full((max_height_, max_widt_h), 0.5, dtype=np.float32)\n",
    "            h = (padded_height_-height)//2\n",
    "            ar[h:h+height, 0:width] = im_ar\n",
    "            im_ar = ar\n",
    "\n",
    "        return im_ar\n",
    "\n",
    "    def next(self):\n",
    "        df_batch = Shuffler.next(self)[['image', 'height', 'width']]\n",
    "        im_batch = []\n",
    "        for image in df_batch.image.itertuples():\n",
    "            im_batch.append(self._get_image_array(os.path.join(self._image_dir, image[0]), row[1], row[2], self._im_dim[0], self._im_dim[1]))\n",
    "            \n",
    "        return np.asarray(im_batch)\n",
    "\n",
    "class FormulaIterator(ShuffleIterator):\n",
    "    def __init__(self, df_, batch_size_, data_dir_, seq_filename_):\n",
    "        Shuffler.__init__(self, df_, batch_size_)\n",
    "        self._seq_data = pd.read_pickle(os.path.join(data_dir_, seq_filename_))\n",
    "        \n",
    "    def next(self):\n",
    "        df_batch = Shuffler.next(self).bin_len\n",
    "        bin_len = df_batch.iloc[0].bin_len\n",
    "        return self._seq_data[bin_len][df_batch.index].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Input(shape=PARAMS.image_shape, name='image')\n",
    "convnet = VGG16(include_top=False, weights='imagenet', input_tensor=image, pooling=None, input_shape=PARAMS.image_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
