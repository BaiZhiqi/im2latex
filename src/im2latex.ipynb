{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# im2latex(S): Deep Learning Model\n",
    "\n",
    "&copy; Copyright 2017 Sumeet S Singh\n",
    "\n",
    "    This file is part of the im2latex solution (by Sumeet S Singh in particular since there are other solutions out there).\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the Affero GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    Affero GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the Affero GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "* Follows the [Show, Attend and Tell paper](https://www.semanticscholar.org/paper/Show-Attend-and-Tell-Neural-Image-Caption-Generati-Xu-Ba/146f6f6ed688c905fb6e346ad02332efd5464616)\n",
    "* [VGG ConvNet (16 or 19)](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) without the top-3 layers\n",
    "    * Pre-initialized with the VGG weights but allowed to train\n",
    "    * The ConvNet outputs $D$ dimensional vectors in a WxH grid where W and H are 1/16th of the input image size (due to 4 max-pool layers). Defining $W.H \\equiv L$ the ConvNet output represents L locations of the image $i \\in [1,L]$ and correspondingly outputs to L annotation vectors $a_i$, each of size $D$.\n",
    "* A dense (FC) attention model: The deterministic soft-attention model of the paper computes $\\alpha_{t,i}$ which is used to select or blend the $a_i$ vectors before being fed as inputs to the decoder LSTM network (see below).\n",
    "    * Inputs to the attention model are $a_i$ and $h_{t-1}$ (previous hidden state of LSTM network - see below)\n",
    "    and $$\\alpha_{t,i} = softmax ( f_{att}(a_i, h_{t-1}) )$$\n",
    "* A Decoder model: A conditioned LSTM that outputs probabilities of the text tokens $y_t$ at each step. The LSTM is conditioned upon $z_t = \\sum_i^L(\\alpha_{t,i}.a_i)$ and takes the previous hidden state $h_{t-1}$ as input. In addition, an embedding of the previous output $Ey_{t-1}$ is also input to the LSTM. At training time, $y_{t-1}$ would be derived from the training samples, while at inferencing time it would be fed-back from the previous predicted word.\n",
    "    * $y$ is taken from a fixed vocabulary of K words. An embedding matrix $E$ is used to narrow its representation. The embedding weights $E$ are learnt end-to-end by the model as well.\n",
    "    * The decoder LSTM uses a deep layer between $h_t$ and $y_t$. It is called a deep output layer and is described in [section 3.2.2 of this paper](https://www.semanticscholar.org/paper/How-to-Construct-Deep-Recurrent-Neural-Networks-Pascanu-G%C3%BCl%C3%A7ehre/533ee188324b833e059cb59b654e6160776d5812). That is:\n",
    "    $$ p(y_t) = Softmax \\Big( f_out(Ey_{t-1}, h_t, \\hat{z}_t) \\Big) $$\n",
    "* Initialization MLPs: Two MLPs are used to produce the initial memory-state of the LSTM as well as $h_{t-1}$ value. Each MLP takes in the entire image's features (i.e. average of $a_i$) as its input and is trained end-to-end.\n",
    "    $$ c_o = f_{init,c}\\Big( \\sum_i^L a_i \\Big) $$\n",
    "    $$ h_o = f_{init,h}\\Big( \\sum_i^L a_i \\Big) $$\n",
    "* Training:\n",
    "    * 3 components above - i.e. all except the conv-net - are trained end-to-end using SGD\n",
    "    * The model is trained for a variable number of time steps - depending on each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Show, Attend and Tell\n",
    "    * [Paper](https://www.semanticscholar.org/paper/Show-Attend-and-Tell-Neural-Image-Caption-Generati-Xu-Ba/146f6f6ed688c905fb6e346ad02332efd5464616)\n",
    "    * [Slides](https://pdfs.semanticscholar.org/b336/f6215c3c15802ca5327cd7cc1747bd83588c.pdf?_ga=2.52116077.559595598.1498604153-2037060338.1496182671)\n",
    "    * [Original Theano code](https://github.com/kelvinxu/arctic-captions)\n",
    "1. [Simonyan, Karen and Andrew Zisserman. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” CoRR abs/1409.1556 (2014): n. pag.](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
    "1. [im2latex solution of Harvard NLP](http://lstm.seas.harvard.edu/latex/)\n",
    "1. [im2latex-dataset tools forked from Harvard NLP](https://github.com/untrix/im2latex-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import dl_commons as dlc\n",
    "import tensorflow as tf\n",
    "from dl_commons import PD \n",
    "from dl_commons import mandatory\n",
    "from dl_commons import instanceof\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Input, Embedding\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import keras\n",
    "import threading\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder = '../data/generated2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab_size(data_dir_):\n",
    "    df_vocab = pd.read_pickle(os.path.join(data_folder, 'df_vocab.pkl'))\n",
    "    return df_vocab.id.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class tensorshape(object):\n",
    "    \"\"\"Tensor shape validator to go with ParamDesc\"\"\"\n",
    "    def __init__(self, shape):\n",
    "        self._shape = shape\n",
    "    def __contains__(self, obj):\n",
    "        return keras.backend.int_shape(obj) == self._shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'att_a_layers': 1, 'B': 128, 'D': None, 'att_a_share_weights': True, 'att_activation': 'tanh', 'm': 64, 'L': None, 'n': 1000, 'att_a_1_n': None, 'att_h_layers': 1, 'K': 556, 'att_h_1_n': None, 'image_shape': (120, 1075, 3)}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del HYPER\n",
    "except:\n",
    "    pass\n",
    "\n",
    "HYPER = dlc.HyperParams((\n",
    "        PD('image_shape',\n",
    "           'Shape of input images. Should be a python sequence.',\n",
    "           None,\n",
    "           (120,1075,3)\n",
    "           ),\n",
    "        PD('B',\n",
    "           '(integer): Size of mini-batch for training, validation and testing.',\n",
    "           instanceof(int),\n",
    "           128\n",
    "           ),\n",
    "        PD('K',\n",
    "           'Vocabulary size including zero',\n",
    "           range(500,1000),\n",
    "           get_vocab_size(data_folder)\n",
    "           ),\n",
    "        PD('m',\n",
    "           '(integer): dimensionality of the embedded input vector (i.e. Ey)', \n",
    "           instanceof(int),\n",
    "           64\n",
    "           ),\n",
    "        PD('L',\n",
    "           '(integer): number of pixels in an image feature-map = WxD (see paper or model description)', \n",
    "           instanceof(int)),\n",
    "        PD('D', \n",
    "           '(integer): number of features coming out of the conv-net. Depth/channels of the last conv-net layer. See paper or model description.', \n",
    "           instanceof(int)),\n",
    "        PD('n',\n",
    "           '(integer): Number of hidden-units of the LSTM cell',\n",
    "           instanceof(int),\n",
    "           1000),\n",
    "    \n",
    "    ### Attention Model Params ###\n",
    "        PD('att_activation', 'Activation used in the attention model',\n",
    "           ('tanh'), \n",
    "           'tanh'),\n",
    "        PD('att_a_share_weights', \n",
    "           '(boolean): Flag indicating whether the attention_a model should share weights across locations (L)',\n",
    "           instanceof(bool),\n",
    "           True\n",
    "          ),\n",
    "        PD('att_a_layers', 'Number of layers in the attention_a model', range(1,10), 1),\n",
    "        PD('att_h_layers', 'Number of layers in the attention_h model', range(1,10), 1),\n",
    "        PD('att_a_1_n', 'Number of units in first layer of attention_a model', range(1,10000)),\n",
    "        PD('att_h_1_n', 'Number of units in first layer of attention_h model', range(1,10000))\n",
    "        ))\n",
    "print HYPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Model\n",
    "[VGG ConvNet (16 or 19)](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) without the top-3 layers\n",
    "* Pre-initialized with the VGG weights but allowed to train\n",
    "* The ConvNet outputs $D$ dimensional vectors in a WxH grid where W and H are scaled-down dimensions of the input image size (due to 5 max-pool layers). Defining $W.H \\equiv L$ the ConvNet output represents L locations of the image $i \\in [1,L]$ and correspondingly outputs to L annotation vectors $a_i$, each of size $D$.\n",
    "\n",
    "The conv-net is *not trained* in the original paper and therefore the files can be separately preprocessed and their outputs directly fed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convnet output_shape =  (None, 3, 33, 512)\n"
     ]
    }
   ],
   "source": [
    "## Conv-net\n",
    "# K.set_image_data_format('channels_last')\n",
    "image_input = Input(shape=HYPER.image_shape, name='image_input')\n",
    "convnet = VGG16(include_top=False, weights='imagenet', pooling=None, input_shape=HYPER.image_shape)\n",
    "convnet.trainable = False\n",
    "print 'convnet output_shape = ', convnet.output_shape\n",
    "a = convnet(image_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'att_a_layers': 1, 'B': 128, 'D': 512, 'att_a_share_weights': True, 'att_activation': 'tanh', 'm': 64, 'L': 99, 'n': 1000, 'att_a_1_n': 512, 'att_h_layers': 1, 'K': 556, 'att_h_1_n': 512, 'image_shape': (120, 1075, 3)}\n"
     ]
    }
   ],
   "source": [
    "HYPER.L = K.int_shape(a)[1]*K.int_shape(a)[2]\n",
    "HYPER.D = K.int_shape(a)[3]\n",
    "HYPER.att_a_1_n = HYPER.D\n",
    "HYPER.att_h_1_n = HYPER.D\n",
    "print HYPER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Input Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def make_batch_list(df_, batch_size_):\n",
    "    ## Make a list of batches\n",
    "    bin_lens = sorted(df_.bin_len.unique())\n",
    "    bin_counts = [df_[df_.bin_len==l].shape[0] for l in bin_lens]\n",
    "    batch_list = []\n",
    "    for i in range(len(bin_lens)):\n",
    "        bin_ = bin_lens[i]\n",
    "        num_batches = (bin_counts[i] // batch_size_)\n",
    "        ## Just making sure bin size is integral multiple of batch_size.\n",
    "        ## This is not a requirement for this function to operate, rather\n",
    "        ## is a way of possibly catching data-corrupting bugs\n",
    "        assert (bin_counts[i] % batch_size_) == 0\n",
    "        batch_list.extend([(bin_, j) for j in range(num_batches)])\n",
    "\n",
    "    np.random.shuffle(batch_list)\n",
    "    return batch_list\n",
    "\n",
    "class ShuffleIterator(object):\n",
    "    def __init__(self, df_, batch_size_):\n",
    "        self._df = df_.sample(frac=1)\n",
    "        self._batch_size = batch_size_\n",
    "        self._batch_list = make_batch_list(self._df, batch_size_)\n",
    "        self._next_pos = 0\n",
    "        self._num_items = (df_.shape[0] // batch_size_)\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "#     def __iter__(self):\n",
    "#         return self\n",
    "    \n",
    "    def next(self):\n",
    "        ## This is an infinite iterator\n",
    "        with self.lock:\n",
    "            if self._next_pos >= self._num_items:\n",
    "                ## Recompose the batch-list\n",
    "                ## Shuffle the samples\n",
    "                self._df = self._df.sample(frac=1)\n",
    "                self._batch_list = make_batch_list(self._df, batch_size_)\n",
    "                self._next_pos %= self._num_items\n",
    "            next_pos = self._next_pos\n",
    "            self._next_pos += 1\n",
    "        \n",
    "        batch = self._batch_list[next_pos]\n",
    "        df_bin = self._df[self._df.bin_len == batch[0]]\n",
    "        assert df_bin.bin_len.iloc[batch[1]*self._batch_size] == batch[0]\n",
    "        assert df_bin.bin_len.iloc[(batch[1]+1)*self._batch_size-1] == batch[0]\n",
    "        return df_bin.iloc[batch[1]*self._batch_size : (batch[1]+1)*self._batch_size]\n",
    "\n",
    "class ImageIterator(ShuffleIterator):\n",
    "    def __init__(self, df_, batch_size_, image_dim_, image_dir_):\n",
    "        Shuffler.__init__(self, df_, batch_size_)\n",
    "        self._im_dim = image_dim_\n",
    "        self._image_dir = image_dir_\n",
    "\n",
    "    @staticmethod\n",
    "    def get_image_matrix(image_path_, height_, width_, padded_height_, padded_width_):\n",
    "        MAX_PIXEL = 255.0 # Ensure this is a float literal\n",
    "        ## Load image and convert to a 3-channel array\n",
    "        im_ar = ndimage.imread(os.path.join(image_dir_,sr_row_.image), mode='RGB')\n",
    "        ## normalize values to lie between -1.0 and 1.0.\n",
    "        ## This is done in place of data whitening - i.e. normalizing to mean=0 and std-dev=0.5\n",
    "        ## Is is a very rough technique but legit for images\n",
    "        im_ar = (im_ar - MAX_PIXEL/2.0) / MAX_PIXEL\n",
    "        height, width = im_ar.shape\n",
    "        assert height == height\n",
    "        assert width == width\n",
    "        if (height < padded_height_) or (width < padded_width_):\n",
    "            ar = np.full((max_height_, max_widt_h), 0.5, dtype=np.float32)\n",
    "            h = (padded_height_-height)//2\n",
    "            ar[h:h+height, 0:width] = im_ar\n",
    "            im_ar = ar\n",
    "\n",
    "        return im_ar\n",
    "\n",
    "    def next(self):\n",
    "        df_batch = Shuffler.next(self)[['image', 'height', 'width']]\n",
    "        im_batch = []\n",
    "        for image in df_batch.image.itertuples():\n",
    "            im_batch.append(self._get_image_array(os.path.join(self._image_dir, image[0]), row[1], row[2], self._im_dim[0], self._im_dim[1]))\n",
    "            \n",
    "        return np.asarray(im_batch)\n",
    "\n",
    "class FormulaIterator(ShuffleIterator):\n",
    "    def __init__(self, df_, batch_size_, data_dir_, seq_filename_):\n",
    "        Shuffler.__init__(self, df_, batch_size_)\n",
    "        self._seq_data = pd.read_pickle(os.path.join(data_dir_, seq_filename_))\n",
    "        \n",
    "    def next(self):\n",
    "        df_batch = Shuffler.next(self).bin_len\n",
    "        bin_len = df_batch.iloc[0].bin_len\n",
    "        return self._seq_data[bin_len][df_batch.index].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_input = Input(shape=(None,), dtype='int32', name='sequence_input')\n",
    "# embedding_output = Embedding(HYPER.vocab_size, HYPER.embedding_size, mask_zero=True, name='embedding')(sequence_input)\n",
    "#model = Model(inputs=[sequence_input], outputs=[embedding_output])\n",
    "#model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "#model.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Model\n",
    "A dense (FC) attention model: The deterministic soft-attention model of the paper computes $\\alpha_{t,i}$ which is used to select or blend the $a_i$ vectors before being fed as inputs to the decoder LSTM network (see below).\n",
    "* Inputs to the attention model are $a_i$ and $h_{t-1}$ (previous hidden state of LSTM network - see below) and $$\\alpha_{t,i} = softmax ( f_{att}(a_i, h_{t-1}) )$$\n",
    "* Note that the model $f_{att}$ shares weights across all values of a_i (i.e. for all i = 1-L). Therefore the shared weight matrix for all a_i has shape (D, D), while shape of a is (B, L, D) where is B=batch-size. Weight matrix of h_i is separate and has the expected shape (n, D). This sharing of weights across a_i is interesting.\n",
    "\n",
    "A Decoder model: A conditioned LSTM that outputs probabilities of the text tokens $y_t$ at each step. The LSTM is conditioned upon $z_t = \\sum_i^L(\\alpha_{t,i}.a_i)$ and takes the previous hidden state $h_{t-1}$ as input. In addition, an embedding of the previous output $Ey_{t-1}$ is also input to the LSTM. At training time, $y_{t-1}$ would be derived from the training samples, while at inferencing time it would be fed-back from the previous predicted word.\n",
    "* $y$ is taken from a fixed vocabulary of K words. An embedding matrix $E$ is used to narrow its representation to an $m$ dimensional dense vector. The embedding weights $E$ are learnt end-to-end by the model as well.\n",
    "* The decoder LSTM uses a deep layer between $h_t$ and $y_t$. It is called a deep output layer and is described in [section 3.2.2 of this paper](https://www.semanticscholar.org/paper/How-to-Construct-Deep-Recurrent-Neural-Networks-Pascanu-G%C3%BCl%C3%A7ehre/533ee188324b833e059cb59b654e6160776d5812). That is:\n",
    "$$ p(y_t) = Softmax \\Big( f_out(Ey_{t-1}, h_t, \\hat{z}_t) \\Big) $$\n",
    "* Optionally $z_t = \\beta \\sum_i^L(\\alpha_{t,i}.a_i)$ where $\\beta = \\sigma(f_{\\beta}(h_{t-1}))$ is a scalar used to modulate the strength of the context. It turns out that for the original use-case of caption generation, the network would learn to emphasize objects by turning up the value of this scalar when it was focusing on objects. It is not clear at this time whether we'll need this feature for im2latex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionedAttentiveRNN(object):\n",
    "    \"\"\"\n",
    "    One timestep of the decoder model. The entire function can be seen as a complex RNN-cell: \n",
    "    which includes a LSTM stack and an attention model.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def _build_attention_model(self, a, h_prev):\n",
    "        B = HYPER.B\n",
    "        n = HYPER.n\n",
    "        L = HYPER.L\n",
    "        D = HYPER.D\n",
    "        att_actv = HYPER.att_activation\n",
    "        initializer = HYPER.weights_initializer_att\n",
    "\n",
    "        ## For #layers > 1 this will endup being different than the paper's implementation\n",
    "        if HYPER.att_share_weights:\n",
    "            \"\"\"\n",
    "            Here we'll effectively create L MLP stacks all sharing the same weights. Each\n",
    "            stack receives a concatenated vector of a(l) and h as input.\n",
    "\n",
    "            TODO: We could also\n",
    "            use 2D convolution here with a kernel of size (1,D) and stride=1 resulting in\n",
    "            an output dimension of (L,1,depth) or (B, L, 1, depth) including the batch dimension.\n",
    "            That may be more efficient.\n",
    "            \"\"\"\n",
    "            ## h.shape = (B,n). Convert it to (B,1,n) and then broadcast to (B,L,n) in order\n",
    "            ## to concatenate with feature vectors of a whose shape=(B,L,D)\n",
    "            h = K.tile(K.expand_dims(h, axis=1), (1,L,1))\n",
    "            ## Concatenate a and h. Final shape = (B, L, D+n)\n",
    "            ah = K.concatenate(a,h)\n",
    "            dim = D+n\n",
    "            for i in range(HYPER.att_layers) + 1 :\n",
    "                n_units = HYPER['att_%d_n'%(i,)]; assert(n_units <= dim)\n",
    "                ah = Dense(num_units, activation=HYPER.att_actv)(ah)\n",
    "                dim = n_units\n",
    "\n",
    "            ## Below is roughly how it is implemented in the code released by the authors of the paper\n",
    "#                 for i in range(1, HYPER.att_a_layers+1):\n",
    "#                     a = Dense(HYPER['att_a_%d_n'%(i,)], activation=HYPER.att_actv)(a)\n",
    "#                 for i in range(1, HYPER.att_h_layers+1):\n",
    "#                     h = Dense(HYPER['att_h_%d_n'%(i,)], activation=HYPER.att_actv)(h)    \n",
    "#                ah = a + K.expand_dims(h, axis=1)\n",
    "\n",
    "            ## Gather all activations across the features; go from (B, L, x) to (B,L,1).\n",
    "            ## One could've just summed\n",
    "            ## them all here, but the paper uses another set of weights to accomplish this.\n",
    "            if HYPER.att_weighted_gather:\n",
    "                ah = Dense(1, activation='linear')(ah) # output shape = (B, L, 1)\n",
    "                ah = K.reshape(ah, (B,L))\n",
    "            else:\n",
    "                ah = K.mean(ah, axis=2) # output shape = (B, L)\n",
    "\n",
    "        else: # weights not shared\n",
    "            ## concatenate a and h_prev and pass them through a MLP. This is different than the theano\n",
    "            ## implementation of the paper because we flatten a from (B,L,D) to (B,L*D). Hence each element\n",
    "            ## of the L*D vector receives its own weight because the effective weight matrix here would be\n",
    "            ## shape (L*D, num_dense_units) as compared to (D, num_dense_units) as in the shared_weights case\n",
    "\n",
    "            ## Concatenate a and h. Final shape will be (B, L*D+n)\n",
    "            ah = K.concatenate(K.batch_flatten(a), h)\n",
    "            dim = L*D + n\n",
    "            for i in range(HYPER.att_layers) + 1 :\n",
    "                n_units = HYPER['att_%d_n'%(i,)]; assert(n_units <= dim)\n",
    "                ah = Dense(HYPER['att_%d_n'%(i,)], activation=HYPER.att_actv)(ah)\n",
    "                dim = n_units\n",
    "            ## At this point, ah.shape = (B, dim)\n",
    "            assert dim >= L\n",
    "\n",
    "        alpha = Dense(L, activation='softmax', name='alpha')(ah)\n",
    "        return alpha\n",
    "            \n",
    "    def build_model(self, a):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            a (tensor): The image annotation vectors as described in the Show Attend & Tell paper.\n",
    "                Should be a tensor of shape (B, L, D).\n",
    "        \"\"\"\n",
    "        ## Set and validate all function params\n",
    "        dlc.HyperParams((\n",
    "                        PD('a', '', tensorshape((B, L, D)))\n",
    "                        ), \n",
    "                       initvals=locals())\n",
    "\n",
    "        ## Renaming HyperParams for convenience\n",
    "        B = HYPER.B\n",
    "        n = HYPER.n\n",
    "        L = HYPER.L\n",
    "        D = HYPER.D\n",
    "        m = HYPER.m\n",
    "        K = HYPER.K\n",
    "        att_actv = HYPER.att_activation\n",
    "        e_init = HYPER.embeddings_initializer\n",
    "\n",
    "        ## Create the placeholders\n",
    "        h = K.placeholder(shape=(B, n), name='h_prev')\n",
    "        y = K.placeholder(shape=(B, 1), name='y_prev')\n",
    "        \n",
    "        ################ Attention Model ################\n",
    "        with tf.name_scope('Attention'):\n",
    "            alpha = self.build_attention_model(a, h_prev)\n",
    "        \n",
    "        ################ Soft deterministic attention: z = alpha-weighted mean of a ################\n",
    "        ## (B, L) batch_dot (B,L,D) -> (B, D)\n",
    "        with tf.name_scope('Phi'):\n",
    "            z = K.batch_dot(alpha, a, axes=[1,1], name='phi')\n",
    "        \n",
    "        ################ Build the embedding layer ################\n",
    "        with tf.name_scope('Ey'):\n",
    "            Ey = Embedding(K, m, embeddings_initializer=e_init, mask_zero=True, input_length=1)\n",
    "            Ey = K.reshape(Ey, (B,m))\n",
    "        \n",
    "        ################ Build the LSTM Cell ################\n",
    "        \n",
    "        \n",
    "    def step_function_train(input, states):\n",
    "        \"\"\"\n",
    "        Conforms to step_function required by keras.backend.rnn. Takes in previous states (h, c), the current\n",
    "        input and the image annotations (a) as input and outputs the states and outputs for the current timestep.\n",
    "        Note that input(t) = Ey(t-1) and input(t=0) = Null. When training, the target output is used for Ey\n",
    "        whereas at prediction time (via. beam-search for e.g.) the actual output is used.\n",
    "        Args:\n",
    "            input (tensor): is a input for one time-step. Should be a tensor of shape (batch-size, m) where m is\n",
    "                the dimensionality of the embedded input vector.\n",
    "            states (list of tensors): Same as new_states returned by this function at previous time-step.\n",
    "                states(t) = new_states(t-1).\n",
    "        Returns:\n",
    "            outputs (tensor): The output of the cell. A tensor of shape (batch_size, num_units)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    def __init__(h_prev, Ey_prev, c_prev, a):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_2:0' shape=(3, 8) dtype=float32>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = K.ones(shape=(3, 4))\n",
    "c = K.ones(shape=(3, 4))\n",
    "K.concatenate([b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,1],[2,2],[3,3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
