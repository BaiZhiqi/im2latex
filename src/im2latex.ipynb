{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im2latex Deep Learning Model\n",
    "--------\n",
    "\n",
    "&copy; Copyright 2017 Sumeet S Singh\n",
    "\n",
    "    This file is part of the im2latex solution by Sumeet S Singh.\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the Affero GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    Affero GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the Affero GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The im2latex model\n",
    "* Follows the [Show, Attend and Tell paper](https://www.semanticscholar.org/paper/Show-Attend-and-Tell-Neural-Image-Caption-Generati-Xu-Ba/146f6f6ed688c905fb6e346ad02332efd5464616)\n",
    "* VGG ConvNet (16 or 19) without the top-3 layers\n",
    "    * Pre-initialized with the VGG weights but allowed to train\n",
    "    * The ConvNet outputs $D$ dimensional vectors in a WxH grid where W and H are 1/16th of the input image size (due to 4 max-pool layers). Defining $W.H \\equiv L$ the ConvNet output represents L locations of the image $i \\in [1,L]$ and correspondingly outputs to L annotation vectors $a_i, i = [1,L]$ of size $D$.\n",
    "* A dense (FC) attention model (i.e. deterministic soft-attention of the paper) computes $\\alpha_{t,i}$ which is used to select or blend the $a_i$ vectors before being fed as inputs to the decoder LSTM network (see below).\n",
    "    * Inputs to the attention model are $a_i$ and $h_{t-1}$ (previous hidden state of LSTM network - see below)\n",
    "    and $$\\alpha_{t,i} = softmax ( f_{att}(a_i, h_{t-1}) )$$\n",
    "* A Decoder model: A conditioned LSTM that outputs probabilities of the text tokens $y_t$ at each step. The LSTM is conditioned upon $z_t = \\sum_i^L(\\alpha_{t,i}.a_i)$ and takes the previous hidden state $h_{t-1}$ as input. In addition, an embedding of the previous output $Ey_{t-1}$ is also input to the LSTM. At training time, $y_{t-1}$ would be derived from the training samples, while at inferencing time it would be fed-back from the previous predicted word.\n",
    "    * $y$ is taken from a fixed vocabulary of K words. An embedding matrix $E$ is used to narrow its representation. The embedding weights $E$ are learnt end-to-end by the model as well.\n",
    "    * The decoder LSTM uses a deep layer between $h_t$ and $y_t$. That is:\n",
    "    $$ p(y_t) = Softmax \\Big( MLP(Ey_{t-1}, h_t, \\hat{z}_t) \\Big) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
