{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# im2latex(S): Deep Learning Model\n",
    "\n",
    "&copy; Copyright 2017 Sumeet S Singh\n",
    "\n",
    "    This file is part of the im2latex solution (by Sumeet S Singh in particular since there are other solutions out there).\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the Affero GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    Affero GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the Affero GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "* Follows the [Show, Attend and Tell paper](https://www.semanticscholar.org/paper/Show-Attend-and-Tell-Neural-Image-Caption-Generati-Xu-Ba/146f6f6ed688c905fb6e346ad02332efd5464616)\n",
    "* [VGG ConvNet (16 or 19)](http://www.robots.ox.ac.uk/~vgg/research/very_deep/) without the top-3 layers\n",
    "    * Pre-initialized with the VGG weights but allowed to train\n",
    "    * The ConvNet outputs $D$ dimensional vectors in a WxH grid where W and H are 1/16th of the input image size (due to 4 max-pool layers). Defining $W.H \\equiv L$ the ConvNet output represents L locations of the image $i \\in [1,L]$ and correspondingly outputs to L annotation vectors $a_i, i = [1,L]$ of size $D$.\n",
    "* A dense (FC) attention model: The deterministic soft-attention model of the paper computes $\\alpha_{t,i}$ which is used to select or blend the $a_i$ vectors before being fed as inputs to the decoder LSTM network (see below).\n",
    "    * Inputs to the attention model are $a_i$ and $h_{t-1}$ (previous hidden state of LSTM network - see below)\n",
    "    and $$\\alpha_{t,i} = softmax ( f_{att}(a_i, h_{t-1}) )$$\n",
    "* A Decoder model: A conditioned LSTM that outputs probabilities of the text tokens $y_t$ at each step. The LSTM is conditioned upon $z_t = \\sum_i^L(\\alpha_{t,i}.a_i)$ and takes the previous hidden state $h_{t-1}$ as input. In addition, an embedding of the previous output $Ey_{t-1}$ is also input to the LSTM. At training time, $y_{t-1}$ would be derived from the training samples, while at inferencing time it would be fed-back from the previous predicted word.\n",
    "    * $y$ is taken from a fixed vocabulary of K words. An embedding matrix $E$ is used to narrow its representation. The embedding weights $E$ are learnt end-to-end by the model as well.\n",
    "    * The decoder LSTM uses a deep layer between $h_t$ and $y_t$. It is called a deep output layer and is described in [section 3.2.2 of this paper](https://www.semanticscholar.org/paper/How-to-Construct-Deep-Recurrent-Neural-Networks-Pascanu-G%C3%BCl%C3%A7ehre/533ee188324b833e059cb59b654e6160776d5812). That is:\n",
    "    $$ p(y_t) = Softmax \\Big( f_out(Ey_{t-1}, h_t, \\hat{z}_t) \\Big) $$\n",
    "* Initialization MLPs: Two MLPs are used to produce the initial memory-state of the LSTM as well as $h_{t-1}$ value. Each MLP takes in the entire image's features (i.e. average of $a_i$) as its input and is trained end-to-end.\n",
    "    $$ c_o = f_{init,c}\\Big( \\sum_i^L a_i \\Big) $$\n",
    "    $$ h_o = f_{init,h}\\Big( \\sum_i^L a_i \\Big) $$\n",
    "* Training:\n",
    "    * All 4 components above are trained end-to-end using SGD\n",
    "    * The model is trained for a variable number of time steps - depending on each batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Show, Attend and Tell\n",
    "    * [Paper](https://www.semanticscholar.org/paper/Show-Attend-and-Tell-Neural-Image-Caption-Generati-Xu-Ba/146f6f6ed688c905fb6e346ad02332efd5464616)\n",
    "    * [Slides](file:///Users/sumeet/im2latex/papers/Show,%20Attend%20and%20Tell:%20Neural%20Image%20Caption%20Generation%20with%20Visual%20Attention%20slides.pdf![image.png](attachment:image.png)\n",
    "1. [VGG ConvNet](http://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
    "1. [im2latex solution of Harvard NLP](http://lstm.seas.harvard.edu/latex/)\n",
    "1. [im2latex-dataset tools forked from Harvard NLP](https://github.com/untrix/im2latex-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
