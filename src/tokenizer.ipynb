{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# im2latex(S): Tokenizer\n",
    "\n",
    "&copy; Copyright 2017 Sumeet S Singh\n",
    "\n",
    "    This file is part of im2latex solution by Sumeet S Singh.\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the Affero GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    Affero GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the Affero GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "from IPython.display import display\n",
    "from six.moves import cPickle as pickle\n",
    "import string\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 600\n",
    "pd.options.display.max_columns = 20\n",
    "pd.options.display.max_colwidth = 100\n",
    "pd.options.display.width = 160\n",
    "data_dir = '../data/generated2'\n",
    "image_folder = os.path.join(data_dir,'formula_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Characterset Chart for Reference\n",
    "##### ASCII Control Characters\n",
    "\n",
    "                        CTRL   (^D means to hold the CTRL key and hit d)\n",
    "    Oct  Dec Char  Hex  Key     Comments\n",
    "    \\000   0  NUL  \\x00  ^@ \\0 (Null byte)\n",
    "    \\001   1  SOH  \\x01  ^A    (Start of heading)\n",
    "    \\002   2  STX  \\x02  ^B    (Start of text)\n",
    "    \\003   3  ETX  \\x03  ^C    (End of text) (see: UNIX keyboard CTRL)\n",
    "    \\004   4  EOT  \\x04  ^D    (End of transmission) (see: UNIX keyboard CTRL)\n",
    "    \\005   5  ENQ  \\x05  ^E    (Enquiry)\n",
    "    \\006   6  ACK  \\x06  ^F    (Acknowledge)\n",
    "    \\007   7  BEL  \\x07  ^G    (Ring terminal bell)\n",
    "    \\010   8   BS  \\x08  ^H \\b (Backspace)  (\\b matches backspace inside [] only)\n",
    "                                            (see: UNIX keyboard CTRL)\n",
    "    \\011   9   HT  \\x09  ^I \\t (Horizontal tab)\n",
    "    \\012  10   LF  \\x0A  ^J \\n (Line feed)  (Default UNIX NL) (see End of Line below)\n",
    "    \\013  11   VT  \\x0B  ^K    (Vertical tab)\n",
    "    \\014  12   FF  \\x0C  ^L \\f (Form feed)\n",
    "    \\015  13   CR  \\x0D  ^M \\r (Carriage return)  (see: End of Line below)\n",
    "    \\016  14   SO  \\x0E  ^N    (Shift out)\n",
    "    \\017  15   SI  \\x0F  ^O    (Shift in)\n",
    "    \\020  16  DLE  \\x10  ^P    (Data link escape)\n",
    "    \\021  17  DC1  \\x11  ^Q    (Device control 1) (XON) (Default UNIX START char.)\n",
    "    \\022  18  DC2  \\x12  ^R    (Device control 2)\n",
    "    \\023  19  DC3  \\x13  ^S    (Device control 3) (XOFF)  (Default UNIX STOP char.)\n",
    "    \\024  20  DC4  \\x14  ^T    (Device control 4)\n",
    "    \\025  21  NAK  \\x15  ^U    (Negative acknowledge)  (see: UNIX keyboard CTRL)\n",
    "    \\026  22  SYN  \\x16  ^V    (Synchronous idle)\n",
    "    \\027  23  ETB  \\x17  ^W    (End of transmission block)\n",
    "    \\030  24  CAN  \\x18  ^X    (Cancel)\n",
    "    \\031  25  EM   \\x19  ^Y    (End of medium)\n",
    "    \\032  26  SUB  \\x1A  ^Z    (Substitute character)\n",
    "    \\033  27  ESC  \\x1B  ^[    (Escape)\n",
    "    \\034  28  FS   \\x1C  ^\\    (File separator, Information separator four)\n",
    "    \\035  29  GS   \\x1D  ^]    (Group separator, Information separator three)\n",
    "    \\036  30  RS   \\x1E  ^^    (Record separator, Information separator two)\n",
    "    \\037  31  US   \\x1F  ^_    (Unit separator, Information separator one)\n",
    "    \\177 127  DEL  \\x7F  ^?    (Delete)  (see: UNIX keyboard CTRL)\n",
    "    \n",
    "    string.printable = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'\n",
    "    string.whitespace = '\\t\\n\\x0b\\x0c\\r '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102180, 102180)\n"
     ]
    }
   ],
   "source": [
    "def loadImageList(filepath):\n",
    "    df=pd.read_table(filepath, header=None, \n",
    "                     names=['id', 'name', 'type'], \n",
    "                     delim_whitespace=True, \n",
    "                     usecols=('id','name'), \n",
    "                     dtype={'id':int, 'name':str, 'type':str})\n",
    "    return df\n",
    "\n",
    "def getImageDetails(data_dir):\n",
    "    try:\n",
    "        image_details = pd.read_csv(os.path.join(data_dir,'image_details.csv'),\n",
    "                                   index_col=0)\n",
    "        return image_details\n",
    "    except:\n",
    "        pass\n",
    "    widths=[]\n",
    "    heights=[]\n",
    "    filenames=[]\n",
    "    imageList = loadImageList(os.path.join(data_dir,'im2latex.lst'))\n",
    "    for i in range(imageList.shape[0]):\n",
    "        try:\n",
    "            image_name = imageList.iloc[i,1] + '.png'\n",
    "            im = Image.open(os.path.join(image_folder,image_name))\n",
    "            widths.append(im.size[0])\n",
    "            heights.append(im.size[1])\n",
    "            filenames.append(image_name)\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            pass\n",
    "    print(len(widths), len(filenames), len(heights))\n",
    "    dff = pd.DataFrame({'filename':filenames, 'width':widths, 'height':heights})\n",
    "    dff.to_csv(os.path.join(data_dir,'image_details.csv'))\n",
    "    return dff\n",
    "\n",
    "\n",
    "def getDatasetDetails(data_dir):\n",
    "    try:\n",
    "        image_details = pd.read_pickle(os.path.join(data_dir,'df_image_details.pkl'))\n",
    "        return image_details\n",
    "    except:\n",
    "        pass\n",
    "    widths=[]\n",
    "    heights=[]\n",
    "    formula_lens=[]\n",
    "    datasetDF = pd.read_pickle(os.path.join(data_dir,'im2latex_map.pkl'))\n",
    "    for _, row in datasetDF.iterrows():\n",
    "        image_name = row.image\n",
    "        im = Image.open(os.path.join(image_folder,image_name))\n",
    "        widths.append(im.size[0])\n",
    "        heights.append(im.size[1])\n",
    "        formula_lens.append(len(row.latex))\n",
    "    print(len(widths), len(heights))\n",
    "    datasetDF = datasetDF.assign(width=widths, height=heights, formula_len=formula_lens)\n",
    "    datasetDF.to_pickle(os.path.join(data_dir,'df_image_details.pkl'))\n",
    "    return datasetDF\n",
    "\n",
    "df_image_details = getDatasetDetails(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_df_clean(data_dir_, df_image_details_):\n",
    "    NONPRINTABLE_CHARS_RE = r'[^\\\\' + string.printable + r']'\n",
    "    DELETE_RE = re.compile(r\".\\x7F\")\n",
    "    PERCENTS_RE = r'%'\n",
    "    try:\n",
    "        return pd.read_pickle(os.path.join(data_dir_,'df_clean.pkl'))\n",
    "    except Exception as e:\n",
    "        print e    \n",
    "        df = df_image_details_\n",
    "        # Make sure everything's ascii\n",
    "        # Coalesce whitespace to a single space\n",
    "        # Strip whitespace from the sides\n",
    "        # Strip percent signs from the sides\n",
    "        # Discard strings with non-printable characters\n",
    "        # Discard strings with embedded percent signs (because textogif ignores everything after the % sign)\n",
    "        cleaned = df.latex.str.decode('ascii').str.encode('ascii').str.replace(r\"\\s+\", ' ').str.strip().str.strip('%')\n",
    "        df = df.assign(latex_ascii=cleaned, latex_ascii_len=cleaned.str.len())\n",
    "        bad1 = df.latex.str.contains(NONPRINTABLE_CHARS_RE)\n",
    "        print 'nonprintables #: ', bad1.shape\n",
    "        bad2 = df.latex.str.contains(PERCENTS_RE)\n",
    "        print 'percents #: ', bad1.shape\n",
    "        good = ~(bad1 | bad2)\n",
    "        print 'good #: ', good.shape\n",
    "        df = df[good]\n",
    "        df.to_pickle((os.path.join(data_dir_,'df_clean.pkl')))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '../data/generated2/df_clean.pkl'\n",
      "nonprintables #:  (102180,)\n",
      "percents #:  (102180,)\n",
      "good #:  (102180,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99600, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_clean = get_df_clean(data_dir, df_image_details)\n",
    "display(df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TokenDict(object):\n",
    "    def __init__(self):\n",
    "        self._tokens = {}\n",
    "    \n",
    "    def account(self, token_list):\n",
    "        for token in token_list:\n",
    "            self._count(token)\n",
    "            \n",
    "    def _count(self, token):\n",
    "        if token in self._tokens:\n",
    "            self._tokens[token] += 1\n",
    "        else:\n",
    "            self._tokens[token] = 1\n",
    "        return 1\n",
    "    \n",
    "    @property\n",
    "    def dict(self):\n",
    "        return self._tokens\n",
    "    \n",
    "    @property\n",
    "    def tokens(self):\n",
    "        return sorted(self._tokens.keys())\n",
    "            \n",
    "def get_vocabulary(df_, data_dir_):\n",
    "    try:\n",
    "        df_vocab = pd.read_pickle(os.path.join(data_dir_,'df_vocab.pkl'))\n",
    "        df_tokenized = pd.read_pickle(os.path.join(data_dir_,'df_tokenized.pkl'))\n",
    "    except Exception as e:\n",
    "        print e\n",
    "        ## Split latex into tokens. Isolate latex commands first - i.e.\n",
    "        ## (optionally even number of backslashes) followed by one backslash followed by letters.\n",
    "        ## Everything else is a one-character token in itself.\n",
    "        LATEX_RE = re.compile(r\"(?:(?<=\\\\\\\\\\\\\\\\\\\\\\\\)\\\\[a-zA-Z]+)|(?:(?<=\\\\\\\\\\\\\\\\)\\\\[a-zA-Z]+)|(?:(?<=\\\\\\\\)\\\\[a-zA-Z]+)|(?:(?<!\\\\)\\\\[a-zA-Z]+)|.\")\n",
    "        sr_token = df_.latex_ascii.str.findall(LATEX_RE)\n",
    "        df_tokenized = df_.assign(latex_tokenized=sr_token)\n",
    "        ## Aggregate the tokens\n",
    "        vocab = TokenDict()\n",
    "        sr_token.agg(lambda l: vocab.account(l))\n",
    "        ## Sort and save\n",
    "        tokens = []; \n",
    "        count = []\n",
    "        for t in vocab.tokens:\n",
    "            tokens.append(t)\n",
    "            count.append(vocab.dict[t])\n",
    "        ## Assign token-ids. Start with 1. Reserve 0 as a 'null' token.\n",
    "        df_vocab = pd.DataFrame({'id':range(1,len(tokens)+1), 'freq':count}, index=tokens, columns=['id', 'freq'])\n",
    "        ## Persist to disk\n",
    "        df_vocab.to_pickle(os.path.join(data_dir_,'df_vocab.pkl'))\n",
    "        df_tokenized.to_pickle(os.path.join(data_dir_,'df_tokenized.pkl'))\n",
    "        \n",
    "    return df_vocab, df_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '../data/generated2/df_vocab.pkl'\n"
     ]
    }
   ],
   "source": [
    "df_vocab, df_tokenized = get_vocabulary(df_clean, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_vocab[df_vocab.index.str.contains(r'\\\\')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print df_clean.latex_ascii[df_clean.latex.str.contains(r'\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\')].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word2id(df_tokenized_, df_vocab_, data_dir_):\n",
    "    try:\n",
    "        return pd.read_pickle(os.path.join(data_dir, 'df_word2id.pkl'))\n",
    "    except Exception as e:\n",
    "        print e        \n",
    "        word2id = df_vocab_.id.to_dict()\n",
    "        sr_word2id = df_tokenized_.latex_tokenized.apply(lambda l: map(lambda t: word2id[t], l))\n",
    "        df_ = df_tokenized_.assign(word2id=sr_word2id, word2id_len=sr_word2id.str.len())\n",
    "        df_.to_pickle(os.path.join(data_dir_, 'df_word2id.pkl'))\n",
    "        return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '../data/generated2/df_word2id.pkl'\n"
     ]
    }
   ],
   "source": [
    "df_word2id = get_word2id(df_tokenized, df_vocab, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99600, 11)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_word2id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
