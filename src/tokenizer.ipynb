{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# im2latex(S): Tokenizer\n",
    "\n",
    "&copy; Copyright 2017 Sumeet S Singh\n",
    "\n",
    "    This file is part of im2latex solution by Sumeet S Singh.\n",
    "\n",
    "    This program is free software: you can redistribute it and/or modify\n",
    "    it under the terms of the Affero GNU General Public License as published by\n",
    "    the Free Software Foundation, either version 3 of the License, or\n",
    "    (at your option) any later version.\n",
    "\n",
    "    This program is distributed in the hope that it will be useful,\n",
    "    but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "    Affero GNU General Public License for more details.\n",
    "\n",
    "    You should have received a copy of the Affero GNU General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "from IPython.display import display\n",
    "from six.moves import cPickle as pickle\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '../data/generated2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASCII Control Characters\n",
    "\n",
    "                        CTRL   (^D means to hold the CTRL key and hit d)\n",
    "    Oct  Dec Char  Hex  Key     Comments\n",
    "    \\000   0  NUL  \\x00  ^@ \\0 (Null byte)\n",
    "    \\001   1  SOH  \\x01  ^A    (Start of heading)\n",
    "    \\002   2  STX  \\x02  ^B    (Start of text)\n",
    "    \\003   3  ETX  \\x03  ^C    (End of text) (see: UNIX keyboard CTRL)\n",
    "    \\004   4  EOT  \\x04  ^D    (End of transmission) (see: UNIX keyboard CTRL)\n",
    "    \\005   5  ENQ  \\x05  ^E    (Enquiry)\n",
    "    \\006   6  ACK  \\x06  ^F    (Acknowledge)\n",
    "    \\007   7  BEL  \\x07  ^G    (Ring terminal bell)\n",
    "    \\010   8   BS  \\x08  ^H \\b (Backspace)  (\\b matches backspace inside [] only)\n",
    "                                            (see: UNIX keyboard CTRL)\n",
    "    \\011   9   HT  \\x09  ^I \\t (Horizontal tab)\n",
    "    \\012  10   LF  \\x0A  ^J \\n (Line feed)  (Default UNIX NL) (see End of Line below)\n",
    "    \\013  11   VT  \\x0B  ^K    (Vertical tab)\n",
    "    \\014  12   FF  \\x0C  ^L \\f (Form feed)\n",
    "    \\015  13   CR  \\x0D  ^M \\r (Carriage return)  (see: End of Line below)\n",
    "    \\016  14   SO  \\x0E  ^N    (Shift out)\n",
    "    \\017  15   SI  \\x0F  ^O    (Shift in)\n",
    "    \\020  16  DLE  \\x10  ^P    (Data link escape)\n",
    "    \\021  17  DC1  \\x11  ^Q    (Device control 1) (XON) (Default UNIX START char.)\n",
    "    \\022  18  DC2  \\x12  ^R    (Device control 2)\n",
    "    \\023  19  DC3  \\x13  ^S    (Device control 3) (XOFF)  (Default UNIX STOP char.)\n",
    "    \\024  20  DC4  \\x14  ^T    (Device control 4)\n",
    "    \\025  21  NAK  \\x15  ^U    (Negative acknowledge)  (see: UNIX keyboard CTRL)\n",
    "    \\026  22  SYN  \\x16  ^V    (Synchronous idle)\n",
    "    \\027  23  ETB  \\x17  ^W    (End of transmission block)\n",
    "    \\030  24  CAN  \\x18  ^X    (Cancel)\n",
    "    \\031  25  EM   \\x19  ^Y    (End of medium)\n",
    "    \\032  26  SUB  \\x1A  ^Z    (Substitute character)\n",
    "    \\033  27  ESC  \\x1B  ^[    (Escape)\n",
    "    \\034  28  FS   \\x1C  ^\\    (File separator, Information separator four)\n",
    "    \\035  29  GS   \\x1D  ^]    (Group separator, Information separator three)\n",
    "    \\036  30  RS   \\x1E  ^^    (Record separator, Information separator two)\n",
    "    \\037  31  US   \\x1F  ^_    (Unit separator, Information separator one)\n",
    "    \\177 127  DEL  \\x7F  ^?    (Delete)  (see: UNIX keyboard CTRL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    if chr(i) in string.printable:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    if chr(i) in string.whitespace:\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATEX_RE = re.compile(r\"(?:\\\\[a-zA-Z]+)|.\")\n",
    "DELETE_RE = re.compile(r\".\\x7F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_df_clean(data_dir_):\n",
    "    try:\n",
    "        return pd.read_pickle(os.path.join(data_dir_,'image_details_clean.pkl'))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    def cleanAscii(s):\n",
    "        # Throw exception if any non-ascii characters exist\n",
    "        s = codecs.encode(codecs.decode(s, 'ascii'), 'ascii') # throws\n",
    "        # Process the deletes in the latex formulas - there are a few\n",
    "        s = re.sub(DELETE_RE, '', s)\n",
    "        # Remove non-printable characters (e.g. \\r was found in the data)\n",
    "        ls = [ ''.join(c for c in ss if c in string.printable) for ss in s.split()]\n",
    "        # Normalize continuous whitespaces (i.e. multiple \\t or spaces) with a single space\n",
    "        s = ' '.join(ls)\n",
    "        return pd.Series([s, len(s)], index=['latex_ascii', 'latex_ascii_len'])\n",
    "\n",
    "    def clean(df):\n",
    "        # Throw exception if any non-ascii characters exist\n",
    "        # Process deletes - there are a few\n",
    "        # Replace continuous whitespace with a single space\n",
    "        cleaned = df.latex.str.decode('ascii').str.encode('ascii').str.replace(DELETE_RE, '').str.replace(r\"\\s+\", ' ').str.strip()\n",
    "        return df.assign(latex_ascii=cleaned, latex_ascii_len=cleaned.str.len())\n",
    "        \n",
    "        \n",
    "    #cleaned = df.latex.apply(cleanAscii)\n",
    "    #df_clean = df.assign(latex_ascii=cleaned.latex_ascii, latex_ascii_len=cleaned.latex_ascii_len)\n",
    "    #df_clean.to_pickle(os.path.join(data_dir_,'image_details_clean.pkl'))\n",
    "    #return df_clean\n",
    "    return clean(pd.read_pickle(os.path.join(data_dir_,'image_details.pkl')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_df_clean2(data_dir_):\n",
    "#     try:\n",
    "#         return pd.read_pickle(os.path.join(data_dir_,'image_details_clean.pkl'))\n",
    "#     except:\n",
    "#         pass\n",
    "    \n",
    "    df = pd.read_pickle(os.path.join(data_dir_,'image_details.pkl'))\n",
    "\n",
    "    def cleanAscii(s):\n",
    "        # Throw error if any non-ascii characters exits\n",
    "        s = codecs.encode(codecs.decode(s, 'ascii'), 'ascii') # throws\n",
    "        # Remove ascii control characters (e.g. \\r was found in the data)\n",
    "        s = ' '.join(s.split())\n",
    "        return pd.Series([s, len(s)], index=['latex_ascii', 'latex_ascii_len'])\n",
    "    \n",
    "    cleaned = df.latex.apply(cleanAscii)\n",
    "    df_clean = df.assign(latex_ascii=cleaned.latex_ascii, latex_ascii_len=cleaned.latex_ascii_len)\n",
    "    #df_clean.to_pickle(os.path.join(data_dir_,'image_details_clean.pkl'))\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102180, 8)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_clean = get_df_clean(data_dir)\n",
    "display(df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean2 = get_df_clean2(data_dir)\n",
    "display(df_clean2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff = df_clean.latex != df_clean2.latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_df_dirty(data_dir_, df_clean_):\n",
    "    try:\n",
    "        return pd.read_pickle(os.path.join(data_dir_,'image_details_dirty.pkl'))\n",
    "    except Exception as e:\n",
    "        print('Exception:',e)\n",
    "    \n",
    "    df_dirty = df_clean_[(df_clean_.latex != df_clean_.latex_ascii)]\n",
    "    df_dirty = df_dirty[['latex', 'latex_ascii', 'formula_len', 'latex_ascii_len']]\n",
    "    df_dirty.to_pickle(os.path.join(data_dir_,'image_details_dirty.pkl'))\n",
    "    return df_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_df_dirty2(data_dir_, df_clean_):\n",
    "#     try:\n",
    "#         return pd.read_pickle(os.path.join(data_dir_,'image_details_dirty.pkl'))\n",
    "#     except Exception as e:\n",
    "#         print('Exception:',e)\n",
    "    \n",
    "    df_dirty = df_clean_[(df_clean_.latex != df_clean_.latex_ascii)]\n",
    "    df_dirty = df_dirty[['latex', 'latex_ascii', 'formula_len', 'latex_ascii_len']]\n",
    "    #df_dirty.to_pickle(os.path.join(data_dir_,'image_details_dirty.pkl'))\n",
    "    return df_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dirty = get_df_dirty2(data_dir, df_clean)\n",
    "display(df_dirty.shape)\n",
    "display(df_dirty.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTokensFromStr(s):\n",
    "    return set(re.findall(LATEX_RE, s))\n",
    "\n",
    "def get_vocabulary(data_dir_, df_clean_):\n",
    "    try:\n",
    "        with open(os.path.join(data_dir_, 'vocabulary.pkl'), 'rb') as f:\n",
    "            return pickle.load(os.path.join(data_dir_, 'vocabulary.pkl'))\n",
    "    except Exception as e:\n",
    "        print('Exception:', e)\n",
    "        \n",
    "    tokenS = df_clean_.latex_ascii.apply(extractTokensFromStr)\n",
    "    words = set([])\n",
    "    tokenS.agg(lambda s: words.update(s))\n",
    "#     with open(os.path.join(data_dir_, 'vocabulary.pkl'), 'wb') as f:\n",
    "#         pickle.dump(words, f, pickle.HIGHEST_PROTOCOL)\n",
    "    return words\n",
    "#extractTokensFromStr('xyz\\\\abc  \\tefg{}\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractTokensFromStr2(s):\n",
    "    return set(re.findall(LATEX_RE, ' '.join(s.split())))\n",
    "\n",
    "def get_vocabulary2(data_dir_, df_clean_):\n",
    "#     try:\n",
    "#         with open(os.path.join(data_dir_, 'vocabulary.pkl'), 'rb') as f:\n",
    "#             return pickle.load(os.path.join(data_dir_, 'vocabulary.pkl'))\n",
    "#     except Exception as e:\n",
    "#         print('Exception:', e)\n",
    "        \n",
    "    tokenS = df_clean_.latex_ascii.apply(extractTokensFromStr2)\n",
    "    words = set([])\n",
    "    tokenS.agg(lambda s: words.update(s))\n",
    "#     with open(os.path.join(data_dir_, 'vocabulary.pkl'), 'wb') as f:\n",
    "#         pickle.dump(words, f, pickle.HIGHEST_PROTOCOL)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocabulary = get_vocabulary(data_dir, df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary2 = get_vocabulary2(data_dir, df_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocabulary2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in vocabulary2:\n",
    "    if word not in vocabulary:\n",
    "        print ord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in vocabulary:\n",
    "    if word not in vocabulary2:\n",
    "        print ord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\label{vvvfq}\\\\xi_q:=\\\\delta_v q^i(t) \\\\frac{\\\\partial}{\\\\partial q^i},%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\\\label{lied1}\\\\delta_h F(q^i(t), \\\\dot q^i(t), t)={L}_\\\\xi F(q^i(t), \\\\dot q^i(t), t).%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\\\label{dSD}d_v S_D=\\\\sum_k (t_{k+1}-t_k)d_v {L_D}^{(k)}, \\\\qquad d_v{L_D}^{(k)}={{\\\\cal E}_D}^{(k)}+\\\\Delta_k{\\\\theta_D}^{(k)},'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\\\label{ddsDf}d_v{{\\\\cal E}_D}^{(i,j)}+\\\\Delta_\\\\mu {\\\\omega_D}^{\\\\mu (i,j)}=0,\\\\qquad{\\\\omega_D}^{\\\\mu(i,j)}:=d_v{\\\\theta_D}^{\\\\mu(i,j)}.%=d_v{p_{i}}^{(k+1)}\\\\wedge d_vq^{i(k)}.% .'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\\\label{eleqm}\\\\frac{\\\\partial {L}}{\\\\partial q^i}-\\\\frac{d}{dt}(\\\\frac{\\\\partial{L}}{\\\\partial \\\\dot q^i})=0.%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\\\label{dDt}\\\\delta_t(t_{k+1}-t_k)=\\\\Delta_k(\\\\delta_tt_k)(t_{k+1}-t_k),%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_dirty = df_clean.latex_ascii[df_clean.latex.str.contains(chr(127))]\n",
    "for i in range(s_dirty.shape[0]):\n",
    "    display(s_dirty.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
